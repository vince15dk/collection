usterName:
image:
  repository: sorintlab/stolon
  tag: v0.13.0-pg10
  pullPolicy: IfNotPresent

# used by create-cluster-job when store.backend is etcd
etcdImage:
  repository: k8s.gcr.io/etcd-amd64
  tag: 2.3.7
  pullPolicy: IfNotPresent

debug: false

persistence:
  enabled: true
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  storageClassName: "gp2-manual"
  accessModes:
    - ReadWriteOnce
  size: 10Gi

rbac:
  create: true

serviceAccount:
  create: true
  # The name of the ServiceAccount to use. If not set and create is true, a name is generated using the fullname template
  name:


superuserUsername: "proxy"
## password for the superuser (REQUIRED if superuserSecret is not set)
superuserPassword: ""

replicationUsername: "keeper"
## password for the replication user (REQUIRED if replicationSecret is not set)
replicationPassword: ""

## backend could be one of the following: consul, etcdv2, etcdv3 or kubernetes
store:
  backend: kubernetes
#  endpoints: "http://stolon-consul:8500"
  kubeResourceKind: configmap

pgParameters:
   max_connections: "200"
   shared_buffers: "256MB"
   timezone: "Asia/Seoul"
   log_statement: "all"

ports:
  stolon:
    containerPort: 5432
  metrics:
    containerPort: 8080

job:
  autoCreateCluster: true
  autoUpdateClusterSpec: true

clusterSpec:
  synchronousReplication: true
  minSynchronousStandbys: 1
  maxSynchronousStandbys: 1
  initMode: new

keeper:
  uid_prefix: "keeper"
  replicaCount: 3
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
  resources:
    requests:
      cpu: "200m"
      memory: "512Mi"
  priorityClassName: ""
  service:
    type: ClusterIP
    annotations: {}
    ports:
      keeper:
        port: 5432
        targetPort: 5432
        protocol: TCP
  nodeSelector: {}
  affinity: {}
  tolerations: []
  volumes: []
  volumeMounts: []
  hooks:
    failKeeper:
      enabled: false
  podDisruptionBudget:
      minAvailable: 2
    # maxUnavailable: 1

proxy:
  replicaCount: 3
  annotations:
     prometheus.io/scrape: "true"
     prometheus.io/port: "8080"
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
  priorityClassName: ""
  service:
    type: ClusterIP
#    loadBalancerIP: ""
    annotations: {}
    ports:
      proxy:
        port: 5432
        targetPort: 5432
        protocol: TCP
  nodeSelector: {}
  affinity: {}
  tolerations: []
  podDisruptionBudget:
      minAvailable: 2
    # maxUnavailable: 1

sentinel:
  replicaCount: 3
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
  priorityClassName: ""
  nodeSelector: {}
  affinity: {}
  tolerations: []
  podDisruptionBudget:
      minAvailable: 2
    # maxUnavailable: 1

## initdb scripts
## Specify dictionary of scripts to be run at first boot, the entry point script is create_script.sh
## i.e. you can use pgsql to run sql script on the cluster.
##
# initdbScripts:
#   create_script.sh: |
#      #!/bin/sh
#      echo "Do something."

## nodePostStart scripts
## Specify dictionary of scripts to be run at first boot, the entry point script is postStartScript.sh
## i.e. you can create tablespace directory here.
##
# nodePostStartScript:
#  postStartScript.sh: |
#    #!/bin/bash
#     echo "Do something."

